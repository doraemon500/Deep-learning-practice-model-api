{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "043c8ec1-b13d-4a6a-a501-3bb3a97b6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import json \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, precision_score, recall_score\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from numba import cuda\n",
    "from peft import LoraConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4a8b684-02aa-465f-84db-1659ab88283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to DB.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = pymysql.connect(\n",
    "        host='retn0.iptime.org',\n",
    "        port=3306,\n",
    "        user='communav',\n",
    "        password='0D9O2nhyjBhBofrCbQkHZcwnIifJkCjP',\n",
    "        database='communav'\n",
    "    )\n",
    "    print('Successfully connected to DB.')\n",
    "except:\n",
    "    print(f'Failed to connect to DB. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d641a9a3-4743-41d4-9f38-5e40954d92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 299314 articles.\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "    SELECT\n",
    "        articles.id,\n",
    "        articles.title,\n",
    "        articles.text,\n",
    "        articles.category_id\n",
    "    FROM everytime_article_dataset_v1 as articles\n",
    "''')\n",
    "original_articles = cursor.fetchall()\n",
    "print(f'Loaded {len(original_articles)} articles.')\n",
    "# print(original_articles)\n",
    "\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64329e4e-b41e-4374-8bba-cec3dca304af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "datasets = []\n",
    "original_articles = pd.DataFrame(original_articles)\n",
    "original_articles.columns = ['id' , 'title', 'text', 'category_ids']\n",
    "# print(original_articles)\n",
    "\n",
    "NotNandatasets = original_articles[pd.notna(original_articles['category_ids'])]\n",
    "    \n",
    "for i in range(14):\n",
    "    # if i == 0: continue\n",
    "    datasets.append(pd.DataFrame([(article['title'] + article['text'] , 1) if i == int(article['category_ids']) else (article['title'] + article['text'] , 0) for k, article in NotNandatasets.iterrows()] ))\n",
    "    datasets[i].columns = ['content', 'label']\n",
    "    \n",
    "for i in range(14):\n",
    "    true_label = [(data.content, data.label) for data in datasets[i].itertuples() if data.label == 1]\n",
    "    false_label = [(data.content, data.label) for data in datasets[i].itertuples() if data.label == 0]\n",
    "    # false_label = false_label[:len(true_label) * 5]\n",
    "    true_label.extend(false_label)\n",
    "    datasets[i] = pd.DataFrame(true_label, columns=['content', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b03d67d-02f0-49d5-905e-a781bc92fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(articles):\n",
    "    label_counts = [0, 0]\n",
    "    for article in articles.itertuples():\n",
    "        label_counts[article.label] += 1\n",
    "\n",
    "    total_count = sum(label_counts)\n",
    "    percentages = [count / total_count * 100 for count in label_counts]\n",
    "    for i, percentage in enumerate(percentages):\n",
    "        print(f'Label {i}: {percentage:.2f}%')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(['0', '1'], label_counts)\n",
    "    plt.text(0, label_counts[0] + 0.1, str(label_counts[0]), ha='center')\n",
    "    plt.text(1, label_counts[1] + 0.1, str(label_counts[1]), ha='center')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2fea868b-5c15-4899-8a73-24014e1d535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 96.80%\n",
      "Label 1: 3.20%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets[1].columns = ['content', 'label']\n",
    "# df = pd.DataFrame(datasets[1], columns=['content', 'label'])\n",
    "plot_distribution(datasets[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50c0d4c0-a369-40fc-a566-5615577c67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'beomi/KcELECTRA-base-v2022'\n",
    "MAX_LENGTH = 128\n",
    "F_BETA = 0.5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 2e-5\n",
    "SAVE_PATH = r\"C:\\Users\\USER\\Desktop\\model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "affe75b7-f4af-4221-98a8-adcb09b6302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 라벨링 별 train, test set 생성\n",
    "train_val_sets = []\n",
    "for i in range(14):\n",
    "    train_articles, val_articles = train_test_split( datasets[i], test_size=0.2, random_state=42 )\n",
    "    train_val_sets.append((train_articles, val_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "affb80b3-e7ed-466d-83eb-d09e9abaaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(category_id):\n",
    "    train_articles, val_articles = train_val_sets[category_id]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # train_articles = train_articles[:len(val_articles)]\n",
    "    train_contents = [article.content for article in train_articles.itertuples()]\n",
    "    train_labels = [article.label for article in train_articles.itertuples()]\n",
    "    val_contents = [article.content for article in val_articles.itertuples()]\n",
    "    val_labels = [article.label for article in val_articles.itertuples()]\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "        train_contents,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    val_encodings = tokenizer(\n",
    "        val_contents,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "    val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "    # train_dataset = torch.utils.data.TensorDataset(\n",
    "        # train_encodings['input_ids'],\n",
    "        # train_encodings['attention_mask'],\n",
    "        # torch.tensor(train_labels)\n",
    "    # )\n",
    "    # val_dataset = torch.utils.data.TensorDataset(\n",
    "        # val_encodings['input_ids'],\n",
    "        # val_encodings['attention_mask'],\n",
    "        # torch.tensor(val_labels)\n",
    "    # )\n",
    "\n",
    "    # train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_dataset, val_dataset, data_collator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c865a7f0-acf6-4f12-93f6-b2663845684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    prec = precision_score(labels, predictions)\n",
    "    rec = recall_score(labels, predictions)\n",
    "    f_beta = fbeta_score(labels, predictions, beta=F_BETA)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f_beta\": f_beta\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2d06a031-a4f2-482f-a5c6-959223abcfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a95e33e8-8cf9-4289-bf37-8b38e5f946b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "electra.embeddings.word_embeddings.weight \t torch.Size([54343, 768])\n",
      "electra.embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "electra.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "electra.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "electra.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "electra.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "electra.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "electra.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "electra.encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "electra.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "electra.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "classifier.dense.weight \t torch.Size([768, 768])\n",
      "classifier.dense.bias \t torch.Size([768])\n",
      "classifier.out_proj.weight \t torch.Size([2, 768])\n",
      "classifier.out_proj.bias \t torch.Size([2])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 2e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]}]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# 모든 파라미터를 고정\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 레이어의 파라미터만 학습 가능하도록 설정\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# 옵티마이저의 state_dict 출력\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1de49057-bfed-4e52-9326-20c30542fc2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:41, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.551411</td>\n",
       "      <td>0.892895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.341312</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.311107</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.300179</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.291513</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.276998</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.261594</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.242546</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.213760</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.188328</td>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.176229</td>\n",
       "      <td>0.892895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.149513</td>\n",
       "      <td>0.931071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.362745</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.143908</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.879397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.126033</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.887446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.124484</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.883721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.119477</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.874439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.111661</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.880567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.115657</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.885650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.109093</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.874486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.876623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.111001</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.871369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.105366</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.876494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.102211</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.902490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.103401</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.884774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.106412</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.105989</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.901288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.104556</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.099447</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.886454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.101166</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.101008</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.881188</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.879447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.916290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.104548</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.867347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.104737</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.870445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.108064</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.111714</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.904977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.104994</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.861423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.099947</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.902490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.101436</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.104607</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.100173</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.910931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.103116</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.889328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.103289</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.104756</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.102918</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.889328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.106676</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.104229</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.880567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.107391</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.881226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.105969</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.885214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.105715</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.883495</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.885214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.104429</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.881188</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.879447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.104516</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.103930</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.104802</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.105012</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.881188</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.879447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.105623</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.105726</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:35, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.524058</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.273800</td>\n",
       "      <td>0.305201</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.305297</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.305503</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.287000</td>\n",
       "      <td>0.297511</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.306969</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.300455</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.285653</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.281707</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.270131</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.252059</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.221468</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.188140</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.184864</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.143712</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.126523</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.099660</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.863402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.095842</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.883838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.856481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.079389</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.877404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.103008</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.077834</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.905612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.066181</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.867117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.065304</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.064398</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.883028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.060796</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.058086</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.868132</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.881696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.059098</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.925481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.059019</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.878378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.057218</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.057406</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.888158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.055237</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>0.053434</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.897727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.054671</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.888158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.050213</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.904018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.051453</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.914352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.050473</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.887640</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.897727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.049519</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.900901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.899123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.055207</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.864583</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.886752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.047729</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.917431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.048095</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.907080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.046623</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.912162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.047548</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.907080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.047706</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.915179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.910112</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.920455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.049294</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.910088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.048180</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.910088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.052694</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.882979</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.902174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.047254</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.915179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.048700</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.910088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.049242</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.910088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.048602</td>\n",
       "      <td>0.988335</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.915179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:40, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.600271</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>0.514522</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.505511</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.498920</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.484573</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.457880</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.379358</td>\n",
       "      <td>0.792153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.272748</td>\n",
       "      <td>0.832450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.545977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.241089</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.724000</td>\n",
       "      <td>0.923469</td>\n",
       "      <td>0.756689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.170595</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.893574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.155800</td>\n",
       "      <td>0.129114</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.931937</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.927083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.903061</td>\n",
       "      <td>0.953664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.107461</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.963203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.111520</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.903061</td>\n",
       "      <td>0.966157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.089171</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.946502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.090525</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.943878</td>\n",
       "      <td>0.943878</td>\n",
       "      <td>0.943878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.092116</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.973262</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.963983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.930693</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.936255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.081227</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.939698</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.942540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.082790</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.962343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.058200</td>\n",
       "      <td>0.081133</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.968586</td>\n",
       "      <td>0.943878</td>\n",
       "      <td>0.963542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.963918</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.961934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.083967</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.946356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.078654</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.943878</td>\n",
       "      <td>0.967573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.043400</td>\n",
       "      <td>0.075397</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.963918</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.961934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.076914</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>0.964730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.084907</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.933673</td>\n",
       "      <td>0.973404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.073487</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.964103</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.963115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.070033</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.967078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.071299</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.974093</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.971074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.959391</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.960366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.088231</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.989189</td>\n",
       "      <td>0.933673</td>\n",
       "      <td>0.977564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.071543</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.974093</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.971074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.071954</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.974093</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.971074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.072366</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>0.972803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.068736</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.070463</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.085276</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.921951</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.930118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.064313</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.979058</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.973958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.069617</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>0.976891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.065008</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.066312</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.979058</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.973958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.066240</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.065328</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.066248</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.061982</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.061895</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.066422</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.979058</td>\n",
       "      <td>0.954082</td>\n",
       "      <td>0.973958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.061516</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.974227</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.045300</td>\n",
       "      <td>0.063896</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.957661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.065159</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.945545</td>\n",
       "      <td>0.974490</td>\n",
       "      <td>0.951195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.063677</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.950249</td>\n",
       "      <td>0.974490</td>\n",
       "      <td>0.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.058365</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.964467</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.965447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.058712</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.521735</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.326578</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.301567</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.304738</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.308673</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.303499</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.213500</td>\n",
       "      <td>0.316525</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.309883</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.308380</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.225900</td>\n",
       "      <td>0.307981</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.305941</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.301768</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.294607</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.310005</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.295289</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.217100</td>\n",
       "      <td>0.295138</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.262300</td>\n",
       "      <td>0.287789</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.290685</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.288293</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.274807</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.279408</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.205400</td>\n",
       "      <td>0.272225</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.256303</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.248119</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.232895</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.227070</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>0.204536</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.186256</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.169009</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.158877</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.148274</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.132400</td>\n",
       "      <td>0.138278</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.131966</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.130871</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.138666</td>\n",
       "      <td>0.910923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.911983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.114075</td>\n",
       "      <td>0.916225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.240385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.127504</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.107545</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.110524</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.878378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.107141</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.098667</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.873656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.099802</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.913978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.096260</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.883117</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.867347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.096371</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.094186</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.885417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.093377</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.885417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.092338</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.885417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.090025</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.880102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.880102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.088686</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.871212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.087553</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.861650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.088801</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.871212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.088381</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.871212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.505343</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.272372</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.221777</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.221096</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.225786</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.222797</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.229982</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.230622</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>0.222128</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.227831</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.220325</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.217012</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>0.216109</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.217725</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.213688</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.206793</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.201882</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.188561</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.206554</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.184155</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.155521</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.173111</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.112900</td>\n",
       "      <td>0.188459</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.155815</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.134977</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.140564</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.128891</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.119503</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.121263</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.107049</td>\n",
       "      <td>0.943796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.107035</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.103388</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.838150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.097833</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.880829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.097538</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.888325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.087982</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.885167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.082201</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.879828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.078107</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.879828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.084502</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.892019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.078232</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.901288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.076232</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.886076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.072860</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.836576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.072002</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.071217</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.071343</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.071255</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.892116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.070057</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.863454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.071745</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.069633</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.883534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.070904</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.073469</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.892116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.069991</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.072083</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.892116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.071657</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.070693</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.070387</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.877551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:37, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.624200</td>\n",
       "      <td>0.544068</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.378311</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.361644</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.355449</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.354603</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.346309</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.348988</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.340390</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>0.317185</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.287784</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.277376</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.242951</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.213372</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.209900</td>\n",
       "      <td>0.200324</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.179700</td>\n",
       "      <td>0.195501</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>0.206465</td>\n",
       "      <td>0.888653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.192308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.168403</td>\n",
       "      <td>0.921527</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.162020</td>\n",
       "      <td>0.933192</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.737327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.157205</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.779221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.146765</td>\n",
       "      <td>0.950159</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.809129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.130029</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.156759</td>\n",
       "      <td>0.945917</td>\n",
       "      <td>0.839080</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.796943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>0.141264</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.830040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.122750</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.128125</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.123921</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.849421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.145793</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>0.117668</td>\n",
       "      <td>0.962884</td>\n",
       "      <td>0.815126</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.827645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>0.110896</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.869963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.108522</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.890909</td>\n",
       "      <td>0.859649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.104668</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.912621</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.900383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.099395</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.889513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.105509</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.858407</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.862989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.100657</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.892193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.101592</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.894834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.099414</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.898876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.098748</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.894834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.097219</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.905660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.097211</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.932692</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.922053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.099376</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.098601</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.098093</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.095741</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.940594</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.924125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.096052</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.093769</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.969388</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.946215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.089881</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.919540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.093829</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.938735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.092018</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.931373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.093374</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.906593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.092020</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.915094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.092147</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.926641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.093738</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.938735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.938735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.093860</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.938735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:40, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.528853</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.342191</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.281300</td>\n",
       "      <td>0.319736</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.317864</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.316997</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.305827</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.312124</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.298470</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.275788</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.249045</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.201700</td>\n",
       "      <td>0.258753</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.213403</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.200351</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.205767</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.204481</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.182964</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.169382</td>\n",
       "      <td>0.907741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.186916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.156829</td>\n",
       "      <td>0.935313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.158546</td>\n",
       "      <td>0.942736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.774059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.135005</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.799373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.119861</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.823096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.125539</td>\n",
       "      <td>0.962884</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.844327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>0.123852</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.118466</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.883905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.104810</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.085939</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.828157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.082712</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.901099</td>\n",
       "      <td>0.841889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.082681</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.859873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.079471</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.859873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.080453</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.863931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.082295</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.851703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.898004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.077073</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.077383</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.872690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.073525</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.910112</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.906040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.072187</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.922551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.075822</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.077621</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.872690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.072060</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.912088</td>\n",
       "      <td>0.920177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.078142</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.872690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.078122</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.875764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.077252</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.882957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.074372</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.076645</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.890269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.073143</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.907127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.072304</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.907127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.074693</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.885417</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.075247</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.902335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.074555</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.902335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.074870</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.902335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.041900</td>\n",
       "      <td>0.074063</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.907127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.074703</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.899358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.074336</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.907127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.074955</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.899358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.075527</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.899358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.075329</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.899358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>0.530438</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.345804</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.322133</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.301300</td>\n",
       "      <td>0.322453</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.322895</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.306000</td>\n",
       "      <td>0.319763</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.323645</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.324929</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.316895</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.313073</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.303853</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.294513</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.267483</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.238627</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.229347</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.217242</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.202926</td>\n",
       "      <td>0.903499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.052083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.193140</td>\n",
       "      <td>0.923648</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.188025</td>\n",
       "      <td>0.932131</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.215670</td>\n",
       "      <td>0.928950</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.642361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.196314</td>\n",
       "      <td>0.935313</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.683140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.164861</td>\n",
       "      <td>0.937434</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.688776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.175296</td>\n",
       "      <td>0.940615</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.712366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.151475</td>\n",
       "      <td>0.940615</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.143566</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.675182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.193670</td>\n",
       "      <td>0.933192</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.670732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.138483</td>\n",
       "      <td>0.945917</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.705357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>0.137448</td>\n",
       "      <td>0.944857</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.712810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.187640</td>\n",
       "      <td>0.937434</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.699405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.756881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.138867</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.765766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.156542</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.797297</td>\n",
       "      <td>0.641304</td>\n",
       "      <td>0.760309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.133117</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.779070</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>0.768349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.124357</td>\n",
       "      <td>0.954401</td>\n",
       "      <td>0.737864</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.753968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.125950</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.768443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.162279</td>\n",
       "      <td>0.948038</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.757979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.122129</td>\n",
       "      <td>0.954401</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.756048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.126672</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.777311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.132936</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.119406</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.774793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.121295</td>\n",
       "      <td>0.955461</td>\n",
       "      <td>0.760417</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.766807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.125629</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.111700</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.771484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.073800</td>\n",
       "      <td>0.120181</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.790598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.130760</td>\n",
       "      <td>0.955461</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.119697</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.787815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.123910</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.778509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.114866</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.116148</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.772358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.122280</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.786638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.117985</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.113411</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.777559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.836957</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.126414</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.785398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.127899</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.785398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.497852</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.252580</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.194015</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.191759</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.195847</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.194734</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.196269</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.198753</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.192516</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.194166</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.186359</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.179951</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.172206</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.147243</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.132090</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.130828</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.118311</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.131775</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.121483</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.098445</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.100403</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.132340</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.493827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.635838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.662983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.122020</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.114167</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.729927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.084902</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.641026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.118121</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.085563</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.641026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.087357</td>\n",
       "      <td>0.962884</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.614334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.086342</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.649819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.089037</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.640138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.088548</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.087594</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.087729</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.091052</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.649123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.094093</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.087542</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.708812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.093478</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.640138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.089166</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.096977</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.631399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.097929</td>\n",
       "      <td>0.963945</td>\n",
       "      <td>0.587302</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.622896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.095234</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.738397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.094317</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.096208</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.738397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.099944</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.631399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.096275</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.677656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.097074</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.738397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.099578</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.737327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.096646</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.098076</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.659341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.098561</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.658363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.097610</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.667870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.096780</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.669145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:37, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.617600</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.331919</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.305599</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.303491</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.300944</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.296352</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.226100</td>\n",
       "      <td>0.303039</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.297089</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.287015</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.278203</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.232900</td>\n",
       "      <td>0.264867</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.232384</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.203900</td>\n",
       "      <td>0.214556</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.207411</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.192133</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.183014</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.176367</td>\n",
       "      <td>0.908802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.168873</td>\n",
       "      <td>0.928950</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.591398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.163980</td>\n",
       "      <td>0.945917</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.751880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.153926</td>\n",
       "      <td>0.940615</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.675355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.094900</td>\n",
       "      <td>0.147947</td>\n",
       "      <td>0.944857</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.696347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.147733</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.810056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.119900</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>0.957582</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.827815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.144522</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.662791</td>\n",
       "      <td>0.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.135745</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.814917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.130970</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.774648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.132469</td>\n",
       "      <td>0.949099</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.125281</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.746888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.127672</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.829016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>0.119284</td>\n",
       "      <td>0.962884</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.802326</td>\n",
       "      <td>0.794931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.128683</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.119359</td>\n",
       "      <td>0.963945</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.790043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.121703</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.837563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.119029</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>0.765306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.118484</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.119738</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.807860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.837563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.122994</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.767635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.118483</td>\n",
       "      <td>0.967126</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.807860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.114538</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.822222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.115188</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.829596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.118004</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.814978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.123053</td>\n",
       "      <td>0.962884</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.780591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.119609</td>\n",
       "      <td>0.969247</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.822222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.119602</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.849282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.119978</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.845411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.119595</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.841232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.118715</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.837104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.116890</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.117085</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.116376</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.116760</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.116943</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.844749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.504810</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.278535</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.214000</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.237991</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.231376</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.238541</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.236222</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.199800</td>\n",
       "      <td>0.229561</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.229501</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.189700</td>\n",
       "      <td>0.218541</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.206784</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.186547</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.162393</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>0.136245</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.128397</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.143739</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.130556</td>\n",
       "      <td>0.938494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.115463</td>\n",
       "      <td>0.941676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.133107</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.616438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.135530</td>\n",
       "      <td>0.959703</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.702247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.102764</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.102405</td>\n",
       "      <td>0.963945</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.099745</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>0.095494</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.705521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.095571</td>\n",
       "      <td>0.970308</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.751634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.968187</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.755814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.093362</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.761589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.089174</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.771812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.087815</td>\n",
       "      <td>0.972428</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.771812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.089266</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.797872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>0.974549</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.780255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.083777</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.799320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.082991</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.786164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.082606</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.816327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.084894</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.085527</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.790323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.087547</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.766871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.084791</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.766871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.083424</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.766871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.083847</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.796178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.086047</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.810345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.085094</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.794702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.080680</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.766871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.085218</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.794702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.086886</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.810345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.805369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.083378</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.796178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.083905</td>\n",
       "      <td>0.977731</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.811258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.081222</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.796178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.079316</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.786164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.081031</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.796178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.796178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.081758</td>\n",
       "      <td>0.976670</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.800654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:37, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>0.496059</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.247867</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.185700</td>\n",
       "      <td>0.189309</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.186546</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>0.189174</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.188147</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.192023</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.186623</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.188663</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.183668</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.181367</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.180994</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.175773</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.163711</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.155623</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.142752</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.140449</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>0.133049</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.123646</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.121012</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.116029</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.108545</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.104926</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.104299</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.099339</td>\n",
       "      <td>0.953340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.096047</td>\n",
       "      <td>0.966066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.099542</td>\n",
       "      <td>0.965005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.090508</td>\n",
       "      <td>0.973489</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.088411</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.979852</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.082477</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.813953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.080893</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.077935</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.075316</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.843023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.074035</td>\n",
       "      <td>0.980912</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.843023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.070055</td>\n",
       "      <td>0.981972</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.071766</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.067814</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.067041</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.066922</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.767544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.064033</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.063064</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.825472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.062674</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.062308</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.061139</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.060543</td>\n",
       "      <td>0.984093</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.841346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.061761</td>\n",
       "      <td>0.983033</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.059797</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.060447</td>\n",
       "      <td>0.986214</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.059295</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.059042</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.058872</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>0.985154</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:38, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.493101</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.235889</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>0.169541</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.167202</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.166344</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.167450</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.167910</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.162230</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.161956</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.158283</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.155498</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>0.151236</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.146617</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.144485</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.132385</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.119830</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.103540</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.101892</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.090493</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.075598</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.068471</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.081834</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.960764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.048003</td>\n",
       "      <td>0.963945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.050870</td>\n",
       "      <td>0.978791</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.038728</td>\n",
       "      <td>0.989396</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.034284</td>\n",
       "      <td>0.992577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.955414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.046288</td>\n",
       "      <td>0.990456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.939597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.030674</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.982659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.992577</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.955414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.025813</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.992577</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.888325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.023494</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.966851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.059110</td>\n",
       "      <td>0.990456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.939597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.982659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.043449</td>\n",
       "      <td>0.991516</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.947712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.024306</td>\n",
       "      <td>0.994698</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.021869</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.966851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.024833</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.024387</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.022568</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.976331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.966851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.023812</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.022942</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.022734</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.022698</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.960452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base-v2022 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,396,192 || all params: 130,174,498 || trainable%: 1.8407537857376641\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2832' max='2832' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2832/2832 18:37, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>0.241937</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.174880</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.170594</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.172948</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.173197</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.173667</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.173811</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.167522</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.165407</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>0.153047</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.115211</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.076454</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.057556</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.049469</td>\n",
       "      <td>0.958643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.038038</td>\n",
       "      <td>0.993637</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.964912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.037408</td>\n",
       "      <td>0.993637</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.900474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.025270</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.954774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.022622</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.954774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.020192</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.954774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.023004</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.027560</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.954774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.014833</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.954774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.935961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.017118</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.022701</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.023411</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.019539</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.025694</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>0.997879</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.974359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.025212</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.023201</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.026588</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.024370</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.027101</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.027597</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.027429</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.027446</td>\n",
       "      <td>0.996819</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.983607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.027575</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.027782</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>0.995758</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.962567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Checkpoint destination directory C:\\Users\\USER\\Desktop\\model2\\checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24020\\1269956297.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "metrics_rst = []\n",
    "\n",
    "target_modules = \"query,value,output.dense,intermediate.dense,classifier.dense,classifier.out_proj\".split(\",\")\n",
    "lora_config = LoraConfig(\n",
    "        r=16,  # 업데이트 행렬의 순위로, 정수로 표현됩니다. 낮은 순위는 더 적은 학습 가능한 파라미터를 가진 작은 업데이트 행렬을 생성합니다.\n",
    "        lora_alpha=32,  # LoRA 스케일링 팩터입니다.\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,  \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\" \n",
    "        )\n",
    "\n",
    "\n",
    "for id in range(0, 14):\n",
    "    train_datasets, val_datasets, data_collator, tokenizer = make_dataset(id)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
    "    # model.to(device)\n",
    "    model_path = SAVE_PATH + f'\\model-with-LoRA-category-id-' + str(id) + '-LoRA-adaptors'\n",
    "\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    " \n",
    "    # add LoRA adaptor\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=SAVE_PATH,\n",
    "        per_device_train_batch_size=16,  \n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=LEARNING_RATE, \n",
    "        num_train_epochs=12,\n",
    "        logging_dir=f\"{SAVE_PATH}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        report_to=\"tensorboard\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    " \n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_datasets,\n",
    "        eval_dataset=val_datasets,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    lora_weights = {name: param for name, param in model.named_parameters() if 'lora' in name}\n",
    "    torch.save(lora_weights, model_path)\n",
    "\n",
    "    # torch.save(model.lora.state_dict(), model_path)\n",
    "\n",
    "        \n",
    "print('Training finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
